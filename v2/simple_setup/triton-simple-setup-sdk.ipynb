{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Triton Inference Server in AI Platform Prediction Custom Container (Google Cloud SDK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will walk through the process of deploying NVIDIA's Triton Inference Server into AI Platform Prediction Custom Container service in the Direct Model Server mode:\n",
    "\n",
    "![](img/caip_triton_container_diagram_direct.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='[Enter project name - REQUIRED]'\n",
    "REPOSITORY='caipcustom'\n",
    "REGION='us-central1'\n",
    "TRITON_VERSION='20.06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "\n",
    "MODEL_BUCKET='gs://{}-{}'.format(PROJECT_ID,random.randint(10000,99999))\n",
    "ENDPOINT='https://{}-ml.googleapis.com/v1'.format(REGION)\n",
    "TRITON_IMAGE='tritonserver:{}-py3'.format(TRITON_VERSION)\n",
    "CAIP_IMAGE='{}-docker.pkg.dev/{}/{}/{}'.format(REGION,PROJECT_ID,REPOSITORY,TRITON_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='tsaikevin-1236'\n",
    "REPOSITORY='caipcustom'\n",
    "REGION='us-central1'\n",
    "TRITON_VERSION='20.06'\n",
    "\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "\n",
    "MODEL_BUCKET='gs://{}-{}'.format(PROJECT_ID,random.randint(10000,99999))\n",
    "ENDPOINT='https://{}-ml.googleapis.com/v1'.format(REGION)\n",
    "TRITON_IMAGE='tritonserver:{}-py3'.format(TRITON_VERSION)\n",
    "CAIP_IMAGE='{}-docker.pkg.dev/{}/{}/{}'.format(REGION,PROJECT_ID,REPOSITORY,TRITON_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Artifact Registry\n",
    "This will be used to store the container image for the model server Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta artifacts repositories create $REPOSITORY --repository-format=docker --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the container\n",
    "We will make a copy of the Triton container image into the Artifact Registry, where AI Platform Custom Container Prediction will only pull from during Model Version setup. The following steps will download the NVIDIA Triton Inference Server container to your VM, then upload it to your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.06-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:36f94c39221c4e19921d44296690991057bbebbb15f59dacd88e25ff331bd307\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:20.06-py3\n",
      "nvcr.io/nvidia/tritonserver:20.06-py3\n",
      "The push refers to repository [us-central1-docker.pkg.dev/tsaikevin-1236/caipcustom/tritonserver]\n",
      "\n",
      "\u001b[1B7aefd4ea: Preparing \n",
      "\u001b[1Bab22f50a: Preparing \n",
      "\u001b[1B4bb8a14c: Preparing \n",
      "\u001b[1Bc357696a: Preparing \n",
      "\u001b[1B35b111ce: Preparing \n",
      "\u001b[1B422b8a56: Preparing \n",
      "\u001b[1B5c73ed66: Preparing \n",
      "\u001b[1B91761c8c: Preparing \n",
      "\u001b[1Bdcbd0b8f: Preparing \n",
      "\u001b[1B3fad0b37: Preparing \n",
      "\u001b[1Bbca7086a: Preparing \n",
      "\u001b[1Ba1fe0dac: Preparing \n",
      "\u001b[1B16262158: Preparing \n",
      "\u001b[1Bfaf9c798: Preparing \n",
      "\u001b[1B4dd7a77b: Preparing \n",
      "\u001b[1B4f618f62: Preparing \n",
      "\u001b[1B114ab5c3: Preparing \n",
      "\u001b[1Bb7588393: Preparing \n",
      "\u001b[1B7a4b3a0b: Preparing \n",
      "\u001b[1B3708beeb: Preparing \n",
      "\u001b[1Bc2e3c7b1: Preparing \n",
      "\u001b[1B43d8d50a: Preparing \n",
      "\u001b[1B9bd9798f: Preparing \n",
      "\u001b[1B27c9414b: Preparing \n",
      "\u001b[1B4c1700eb: Preparing \n",
      "\u001b[1B46c23e3a: Preparing \n",
      "\u001b[1Bb877a610: Preparing \n",
      "\u001b[1Be28a7437: Preparing \n",
      "\u001b[1Bd0584a68: Preparing \n",
      "\u001b[1Bcd722629: Preparing \n",
      "\u001b[1B7d6d8dba: Preparing \n",
      "\u001b[1Bf0c094ce: Preparing \n",
      "\u001b[1B579ef21b: Preparing \n",
      "\u001b[1Bd3acfe42: Preparing \n",
      "\u001b[1B29219aba: Preparing \n",
      "\u001b[1Be3bdea67: Preparing \n",
      "\u001b[1B7458d04b: Preparing \n",
      "\u001b[1B37a24627: Preparing \n",
      "\u001b[1Bef4a95c3: Preparing \n",
      "\u001b[1Bd2967507: Layer already exists \u001b[38A\u001b[2K\u001b[33A\u001b[2K\u001b[30A\u001b[2K\u001b[26A\u001b[2K\u001b[22A\u001b[2K\u001b[21A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[8A\u001b[2K\u001b[3A\u001b[2K20.06-py3: digest: sha256:36f94c39221c4e19921d44296690991057bbebbb15f59dacd88e25ff331bd307 size: 8683\n"
     ]
    }
   ],
   "source": [
    "!docker pull nvcr.io/nvidia/$TRITON_IMAGE && \\\n",
    " docker tag nvcr.io/nvidia/$TRITON_IMAGE $CAIP_IMAGE && \\\n",
    " docker push $CAIP_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model Artifacts\n",
    "Clone the NVIDIA Triton Inference Server repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'triton-inference-server'...\n",
      "remote: Enumerating objects: 285, done.\u001b[K\n",
      "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
      "remote: Compressing objects: 100% (190/190), done.\u001b[K\n",
      "remote: Total 25484 (delta 149), reused 161 (delta 89), pack-reused 25199\u001b[K\n",
      "Receiving objects: 100% (25484/25484), 14.34 MiB | 23.26 MiB/s, done.\n",
      "Resolving deltas: 100% (18803/18803), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/triton-inference-server.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the GCS bucket where the model artifacts will be copied to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://tsaikevin-1236-90830/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage model artifacts and copy to bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model_repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R triton-inference-server/docs/examples/model_repository/* model_repository/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd triton-inference-server && git checkout r$TRITON_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/caip-triton/v2/simple_setup/triton-inference-server\n",
      "Branch r20.06 set up to track remote branch r20.06 from origin.\n",
      "Switched to a new branch 'r20.06'\n",
      "/home/jupyter/caip-triton/v2/simple_setup\n",
      "total 116\n",
      "-rw-r--r--  1 jupyter jupyter 13652 Oct 19 08:48 get_request_body_simple.py\n",
      "drwxr-xr-x  2 jupyter jupyter  4096 Oct 19 08:48 \u001b[0m\u001b[01;34mimg\u001b[0m/\n",
      "drwxr-xr-x  6 jupyter jupyter  4096 Oct 26 08:10 \u001b[01;34mmodel_repository\u001b[0m/\n",
      "-rw-r--r--  1 jupyter jupyter  1605 Oct 22 18:27 README.md\n",
      "drwxr-xr-x 10 jupyter jupyter  4096 Oct 26 08:11 \u001b[01;34mtriton-inference-server\u001b[0m/\n",
      "-rw-r--r--  1 jupyter jupyter 60610 Oct 26 08:07 triton-simple-setup-rest.ipynb\n",
      "-rw-r--r--  1 jupyter jupyter 21323 Oct 26 08:09 triton-simple-setup-sdk.ipynb\n"
     ]
    }
   ],
   "source": [
    "%cd triton-inference-server\n",
    "!git checkout r$TRITON_VERSION\n",
    "%cd ..\n",
    "%ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ mkdir -p model_repository/resnet50_netdef/1\n",
      "+ wget -O model_repository/resnet50_netdef/1/model.netdef http://download.caffe2.ai.s3.amazonaws.com/models/resnet50/predict_net.pb\n",
      "--2020-10-26 08:11:05--  http://download.caffe2.ai.s3.amazonaws.com/models/resnet50/predict_net.pb\n",
      "Resolving download.caffe2.ai.s3.amazonaws.com (download.caffe2.ai.s3.amazonaws.com)... 52.216.233.11\n",
      "Connecting to download.caffe2.ai.s3.amazonaws.com (download.caffe2.ai.s3.amazonaws.com)|52.216.233.11|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31649 (31K) [binary/octet-stream]\n",
      "Saving to: ‘model_repository/resnet50_netdef/1/model.netdef’\n",
      "\n",
      "model_repository/re 100%[===================>]  30.91K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-10-26 08:11:05 (1.03 MB/s) - ‘model_repository/resnet50_netdef/1/model.netdef’ saved [31649/31649]\n",
      "\n",
      "+ wget -O model_repository/resnet50_netdef/1/init_model.netdef http://download.caffe2.ai.s3.amazonaws.com/models/resnet50/init_net.pb\n",
      "--2020-10-26 08:11:05--  http://download.caffe2.ai.s3.amazonaws.com/models/resnet50/init_net.pb\n",
      "Resolving download.caffe2.ai.s3.amazonaws.com (download.caffe2.ai.s3.amazonaws.com)... 52.216.233.11\n",
      "Connecting to download.caffe2.ai.s3.amazonaws.com (download.caffe2.ai.s3.amazonaws.com)|52.216.233.11|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 128070759 (122M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘model_repository/resnet50_netdef/1/init_model.netdef’\n",
      "\n",
      "model_repository/re 100%[===================>] 122.14M  84.9MB/s    in 1.4s    \n",
      "\n",
      "2020-10-26 08:11:07 (84.9 MB/s) - ‘model_repository/resnet50_netdef/1/init_model.netdef’ saved [128070759/128070759]\n",
      "\n",
      "+ mkdir -p model_repository/inception_graphdef/1\n",
      "+ wget -O /tmp/inception_v3_2016_08_28_frozen.pb.tar.gz https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\n",
      "--2020-10-26 08:11:07--  https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.74.128, 108.177.112.128, 74.125.124.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.74.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 88668554 (85M) [application/gzip]\n",
      "Saving to: ‘/tmp/inception_v3_2016_08_28_frozen.pb.tar.gz’\n",
      "\n",
      "/tmp/inception_v3_2 100%[===================>]  84.56M   106MB/s    in 0.8s    \n",
      "\n",
      "2020-10-26 08:11:08 (106 MB/s) - ‘/tmp/inception_v3_2016_08_28_frozen.pb.tar.gz’ saved [88668554/88668554]\n",
      "\n",
      "+ cd /tmp\n",
      "+ tar xzf inception_v3_2016_08_28_frozen.pb.tar.gz\n",
      "+ mv /tmp/inception_v3_2016_08_28_frozen.pb model_repository/inception_graphdef/1/model.graphdef\n",
      "+ mkdir -p model_repository/densenet_onnx/1\n",
      "+ wget -O model_repository/densenet_onnx/1/model.onnx https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx\n",
      "--2020-10-26 08:11:09--  https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx\n",
      "Resolving contentmamluswest001.blob.core.windows.net (contentmamluswest001.blob.core.windows.net)... 13.88.144.240\n",
      "Connecting to contentmamluswest001.blob.core.windows.net (contentmamluswest001.blob.core.windows.net)|13.88.144.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 32719461 (31M) [application/octet-stream]\n",
      "Saving to: ‘model_repository/densenet_onnx/1/model.onnx’\n",
      "\n",
      "model_repository/de 100%[===================>]  31.20M  16.1MB/s    in 1.9s    \n",
      "\n",
      "2020-10-26 08:11:11 (16.1 MB/s) - ‘model_repository/densenet_onnx/1/model.onnx’ saved [32719461/32719461]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./triton-inference-server/docs/examples/fetch_models.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model_repository/resnet50_netdef/1/model.netdef [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/simple/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/simple/1/model.graphdef [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/resnet50_netdef/1/init_model.netdef [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/inception_graphdef/inception_labels.txt [Content-Type=text/plain]...\n",
      "Copying file://model_repository/densenet_onnx/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/densenet_onnx/densenet_labels.txt [Content-Type=text/plain]...\n",
      "Copying file://model_repository/inception_graphdef/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/densenet_onnx/1/model.onnx [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/inception_graphdef/1/model.graphdef [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/simple_string/config.pbtxt [Content-Type=application/octet-stream]...\n",
      "Copying file://model_repository/simple_string/1/model.graphdef [Content-Type=application/octet-stream]...\n",
      "/ [12/12 files][244.7 MiB/244.7 MiB] 100% Done                                  \n",
      "Operation completed over 12 objects/244.7 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -R model_repository/ $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://tsaikevin-1236-90830/model_repository/densenet_onnx/\n",
      "gs://tsaikevin-1236-90830/model_repository/inception_graphdef/\n",
      "gs://tsaikevin-1236-90830/model_repository/resnet50_netdef/\n",
      "gs://tsaikevin-1236-90830/model_repository/simple/\n",
      "gs://tsaikevin-1236-90830/model_repository/simple_string/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $MODEL_BUCKET/model_repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare request payload\n",
    "\n",
    "To prepare the payload format, we have included a utility get_request_body_simple.py.  To use this utility, install the following library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geventhttpclient in /opt/conda/lib/python3.7/site-packages (1.4.4)\n",
      "Requirement already satisfied: gevent>=0.13 in /opt/conda/lib/python3.7/site-packages (from geventhttpclient) (20.6.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from geventhttpclient) (1.15.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from geventhttpclient) (2020.6.20)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.7/site-packages (from gevent>=0.13->geventhttpclient) (5.1.0)\n",
      "Requirement already satisfied: zope.event in /opt/conda/lib/python3.7/site-packages (from gevent>=0.13->geventhttpclient) (4.4)\n",
      "Requirement already satisfied: greenlet>=0.4.16; platform_python_implementation == \"CPython\" in /opt/conda/lib/python3.7/site-packages (from gevent>=0.13->geventhttpclient) (0.4.16)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from gevent>=0.13->geventhttpclient) (50.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install geventhttpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare non-binary request payload\n",
    "\n",
    "The first model will illustrate a non-binary payload.  The following command will create a KF Serving v2 format non-binary payload to be used with the \"simple\" model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 get_request_body_simple.py -m simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare binary request payload\n",
    "\n",
    "Triton's implementation of KF Serving v2 protocol for binary data appends the binary data after the json body.  Triton requires an additional header for offset:\n",
    "\n",
    "`Inference-Header-Content-Length: [offset]`\n",
    "\n",
    "We have provided a script that will automatically resize the image to the proper size for ResNet-50 [224, 224, 3] and calculate the proper offset.  The following command takes an image file and outputs the necessary data structure to be use with the \"resnet50_netdef\" model.  Please note down this offset as it will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 224, 224)\n",
      "Add Header: Inference-Header-Content-Length: 138\n"
     ]
    }
   ],
   "source": [
    "!python3 get_request_body_simple.py -m image -f triton-inference-server/qa/images/mug.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and deploy Model and Model Version\n",
    "\n",
    "In this section, we will deploy two models:\n",
    "1. Simple model with non-binary data.  KF Serving v2 protocol specifies a json format with non-binary data in the json body itself.\n",
    "2. Binary data model with ResNet-50.  Triton's implementation of binary data for KF Server v2 protocol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model (non-binary data)\n",
    "\n",
    "#### Create Model\n",
    "\n",
    "AI Platform Prediction uses a Model/Model Version Hierarchy, where the Model is a logical grouping of Model Versions.  We will first create the Model.\n",
    "\n",
    "Because the MODEL_NAME variable will be used later to specify the predict route, and Triton will use that route to run prediction on a specific model, we must set the value of this variable to a valid name of a model.  For this section, will use the \"simple\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Created ml engine model [projects/tsaikevin-1236/models/simple].\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create $MODEL_NAME --region $REGION --enable-logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "NAME    DEFAULT_VERSION_NAME\n",
      "simple\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models list --region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Version\n",
    "\n",
    "After the Model is created, we can now create a Model Version under this Model.  Each Model Version will need a name that is unique within the Model.  In AI Platform Prediction Custom Container, a {Project}/{Model}/{ModelVersion} uniquely identifies the specific container and model artifact used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION_NAME='v01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following config file will be used in the Model Version creation command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command with YAML config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_simple={'deploymentUri': MODEL_BUCKET+'/model_repository', \\\n",
    "               'container': {'image': CAIP_IMAGE, \\\n",
    "                             'args': ['tritonserver', '--model-repository=$(AIP_STORAGE_URI)'], \\\n",
    "                             'env': [], \\\n",
    "                             'ports': {'containerPort': 8000}}, \\\n",
    "               'routes': {'predict': '/v2/models/'+MODEL_NAME+'/infer', \\\n",
    "                          'health': '/v2/models/'+MODEL_NAME}, \\\n",
    "               'machineType': 'n1-standard-4', 'autoScaling': {'minNodes': 1}}\n",
    "\n",
    "with open(r'config_simple.yaml', 'w') as file:\n",
    "    config = yaml.dump(config_simple, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create $VERSION_NAME \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --accelerator count=1,type=nvidia-tesla-t4 \\\n",
    "  --config config_simple.yaml \\\n",
    "  --region=$REGION \\\n",
    "  --async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.ai-platform.versions.create) argument VERSION --model: Must be specified.\n",
      "Usage: gcloud beta ai-platform versions create VERSION --model=MODEL [optional flags]\n",
      "  optional flags may be  --accelerator | --args | --async | --command |\n",
      "                         --config | --description | --env-vars |\n",
      "                         --explanation-method | --framework | --health-route |\n",
      "                         --help | --image | --labels | --machine-type |\n",
      "                         --num-integral-steps | --num-paths | --origin |\n",
      "                         --package-uris | --ports | --predict-route |\n",
      "                         --prediction-class | --python-version | --region |\n",
      "                         --runtime-version | --service-account |\n",
      "                         --staging-bucket\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud beta ai-platform versions create --help\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see details of the Model Version just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "acceleratorConfig:\n",
      "  count: '1'\n",
      "  type: NVIDIA_TESLA_T4\n",
      "autoScaling:\n",
      "  minNodes: 1\n",
      "container:\n",
      "  args:\n",
      "  - tritonserver\n",
      "  - --model-repository=$(AIP_STORAGE_URI)\n",
      "  image: us-central1-docker.pkg.dev/tsaikevin-1236/caipcustom/tritonserver:20.06-py3\n",
      "  ports:\n",
      "  - containerPort: 8000\n",
      "createTime: '2020-10-26T08:13:29Z'\n",
      "deploymentUri: gs://tsaikevin-1236-90830/model_repository\n",
      "errorMessage: |\n",
      "  Model server terminated: model server container terminated: exit_code: 1\n",
      "  reason: \"Error\"\n",
      "  started_at {\n",
      "    seconds: 1603701437\n",
      "  }\n",
      "  finished_at {\n",
      "    seconds: 1603701446\n",
      "  }\n",
      "etag: hPxbO3hKDhY=\n",
      "machineType: n1-standard-4\n",
      "name: projects/tsaikevin-1236/models/simple/versions/v01\n",
      "routes:\n",
      "  health: /v2/models/simple\n",
      "  predict: /v2/models/simple/infer\n",
      "state: FAILED\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions describe $VERSION_NAME --model=$MODEL_NAME --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To list all Model Versions and their states in this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "NAME  DEPLOYMENT_URI                              STATE\n",
      "v01   gs://tsaikevin-1236-90830/model_repository  CREATING\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions list --model=$MODEL_NAME --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction using `curl`\n",
    "\n",
    "The \"simple\" model takes two tensors with shape [1,16] and does a couple of basic arithmetic operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST $ENDPOINT/projects/$PROJECT_ID/models/$MODEL_NAME/versions/$VERSION_NAME:predict \\\n",
    "    -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    -d @simple.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction using Using `requests` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open('simple.json', 'r') as s:\n",
    "    data=s.read()\n",
    "    \n",
    "PREDICT_URL = \"{}/projects/{}/models/{}/versions/{}:predict\".format(ENDPOINT, PROJECT_ID, MODEL_NAME, VERSION_NAME)\n",
    "HEADERS = {\n",
    "  'Content-Type': 'application/octet-stream',\n",
    "  'Authorization': 'Bearer {}'.format(os.popen('gcloud auth application-default print-access-token').read().rstrip())\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", PREDICT_URL, headers=HEADERS, data = data).content.decode()\n",
    "\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 model (binary data)\n",
    "\n",
    "#### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_MODEL_NAME='resnet50_netdef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Created ml engine model [projects/tsaikevin-1236/models/resnet50_netdef].\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create $BINARY_MODEL_NAME --region $REGION --enable-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_VERSION_NAME='v01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command with YAML config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_binary={'deploymentUri': MODEL_BUCKET+'/model_repository', \\\n",
    "               'container': {'image': CAIP_IMAGE, \\\n",
    "                             'args': ['tritonserver', '--model-repository=$(AIP_STORAGE_URI)'], \\\n",
    "                             'env': [], \\\n",
    "                             'ports': {'containerPort': 8000}}, \\\n",
    "               'routes': {'predict': '/v2/models/'+BINARY_MODEL_NAME+'/infer', \\\n",
    "                          'health': '/v2/models/'+BINARY_MODEL_NAME}, \\\n",
    "               'machineType': 'n1-standard-4', 'autoScaling': {'minNodes': 1}}\n",
    "\n",
    "with open(r'config_binary.yaml', 'w') as file:\n",
    "    config_binary = yaml.dump(config_binary, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create $BINARY_VERSION_NAME \\\n",
    "  --model $BINARY_MODEL_NAME \\\n",
    "  --accelerator count=1,type=nvidia-tesla-t4 \\\n",
    "  --config config_binary.yaml \\\n",
    "  --region=$REGION \\\n",
    "  --async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see details of the Model Version just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "acceleratorConfig:\n",
      "  count: '1'\n",
      "  type: NVIDIA_TESLA_T4\n",
      "autoScaling:\n",
      "  minNodes: 1\n",
      "container:\n",
      "  args:\n",
      "  - tritonserver\n",
      "  - --model-repository=$(AIP_STORAGE_URI)\n",
      "  image: us-central1-docker.pkg.dev/tsaikevin-1236/caipcustom/tritonserver:20.06-py3\n",
      "  ports:\n",
      "  - containerPort: 8000\n",
      "createTime: '2020-10-26T08:14:00Z'\n",
      "deploymentUri: gs://tsaikevin-1236-90830/model_repository\n",
      "etag: kHgiPLE3j_Y=\n",
      "machineType: n1-standard-4\n",
      "name: projects/tsaikevin-1236/models/resnet50_netdef/versions/v01\n",
      "routes:\n",
      "  health: /v2/models/resnet50_netdef\n",
      "  predict: /v2/models/resnet50_netdef/infer\n",
      "state: CREATING\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions describe $BINARY_VERSION_NAME --model=$BINARY_MODEL_NAME --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To list all Model Versions and their states in this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "NAME  DEPLOYMENT_URI                              STATE\n",
      "v01   gs://tsaikevin-1236-90830/model_repository  CREATING\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions list --model=$BINARY_MODEL_NAME --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction using `curl`\n",
    "\n",
    "Recall the offset value calcuated above.  The binary case has an additional header:\n",
    "\n",
    "`Inference-Header-Content-Length: [offset]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl --request POST $ENDPOINT/projects/$PROJECT_ID/models/$BINARY_MODEL_NAME/versions/$BINARY_VERSION_NAME:predict \\\n",
    "    -k -H \"Content-Type: application/octet-stream\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    -H \"Inference-Header-Content-Length: 138\" \\\n",
    "    --data-binary @payload.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction using Using `requests` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open('payload.dat', 'rb') as s:\n",
    "    data=s.read()\n",
    "    \n",
    "PREDICT_URL = \"{}/projects/{}/models/{}/versions/{}:predict\".format(ENDPOINT, PROJECT_ID, BINARY_MODEL_NAME, BINARY_VERSION_NAME)\n",
    "HEADERS = {\n",
    "  'Content-Type': 'application/octet-stream',\n",
    "  'Inference-Header-Content-Length': '138',\n",
    "  'Authorization': 'Bearer {}'.format(os.popen('gcloud auth application-default print-access-token').read().rstrip())\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", PREDICT_URL, headers=HEADERS, data = data).content.decode()\n",
    "\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Deleting version [v01]......done.                                              \n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions delete $VERSION_NAME --model=$MODEL_NAME --region=$REGION --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Deleting model [simple]...done.                                                \n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models delete $MODEL_NAME --region=$REGION --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Deleting version [v01]......done.                                              \n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform versions delete $BINARY_VERSION_NAME --model=$BINARY_MODEL_NAME --region=$REGION --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-ml.googleapis.com/]\n",
      "Deleting model [resnet50_netdef]...done.                                       \n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models delete $BINARY_MODEL_NAME --region us-central1 --region=$REGION --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m rm -r -f $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf model_repository triton-inference-server *.yaml *.dat *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-cpu.1-15.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-cpu.1-15:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
