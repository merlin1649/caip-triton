{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorrtserver.api import grpc_service_pb2\n",
    "from functools import partial\n",
    "import grpc_gcp_caip_pb2\n",
    "import grpc_image_client\n",
    "import requests\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-v', '--verbose', action=\"store_true\", required=False, default=False)\n",
    "parser.add_argument('-a', '--async', dest=\"async_set\", action=\"store_true\", required=False)\n",
    "parser.add_argument('--streaming', action=\"store_true\", required=False, default=False)\n",
    "parser.add_argument('-m', '--model-name', type=str, required=True)\n",
    "parser.add_argument('-x', '--model-version', type=int, required=False)\n",
    "parser.add_argument('-b', '--batch-size', type=int, required=False, default=1)\n",
    "parser.add_argument('-c', '--classes', type=int, required=False, default=1)\n",
    "parser.add_argument('-s', '--scaling', type=str, choices=['NONE', 'INCEPTION', 'VGG'],required=False, default='NONE')\n",
    "parser.add_argument('-u', '--url', type=str, required=False, default='localhost:8001')\n",
    "#parser.add_argument('-i','--image_filename', type=str, default=None)\n",
    "parser.add_argument('image_filename', type=str, nargs='?', default=None)\n",
    "\n",
    "FLAGS = parser.parse_args(args=['-m','resnet50_netdef',\n",
    "#                                '-u','35.225.226.0:5000',\n",
    "                                '-u','35.202.30.17:5000',\n",
    "#                                '-u','127.0.0.1:5000',\n",
    "                                '-s','INCEPTION',\n",
    "                                '/workspace/images/mug.jpg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating firewall...-Created [https://www.googleapis.com/compute/v1/projects/tsaikevin-inference/global/firewalls/triton].\n",
      "Creating firewall...done.                                                      \n",
      "NAME    NETWORK  DIRECTION  PRIORITY  ALLOW     DENY  DISABLED\n",
      "triton  default  INGRESS    1000      tcp:5000        False\n"
     ]
    }
   ],
   "source": [
    "!gcloud compute --project=tsaikevin-inference firewall-rules create triton --direction=INGRESS --priority=1000 --network=default --action=ALLOW --rules=tcp:5000 --source-ranges=0.0.0.0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://35.202.30.17:5000/v1/models/m/versions/v:predict'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_url = \"http://{}/v1/models/m/versions/v:predict\".format(FLAGS.url)\n",
    "\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'Authorization': 'Bearer `gcloud auth print-access-token`'\n",
    "}\n",
    "predict_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare request for Status gRPC\n",
    "status_request = grpc_service_pb2.StatusRequest(model_name=FLAGS.model_name)\n",
    "caip_request = grpc_gcp_caip_pb2.CaipRequest(request_type=grpc_gcp_caip_pb2.TYPE_STATUS_REQUEST, status_request = status_request)\n",
    "\n",
    "# CaipRequest send, StatusResponse receive\n",
    "response = requests.request(\"POST\", predict_url, headers=headers, data = caip_request.SerializeToString()).content\n",
    "\n",
    "status_response = grpc_service_pb2.StatusResponse().FromString(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model matches our requirements, and get some\n",
    "# properties of the model that we need for preprocessing\n",
    "input_name, output_name, c, h, w, format, dtype = grpc_image_client.parse_model(status_response, FLAGS.model_name, FLAGS.batch_size, FLAGS.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filledRequestGenerator = partial(grpc_image_client.requestGenerator, input_name, output_name, c, h, w, format, dtype, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send requests of FLAGS.batch_size images. If the number of\n",
    "# images isn't an exact multiple of FLAGS.batch_size then just\n",
    "# start over with the first images until the batch is filled.\n",
    "result_filenames = []\n",
    "infer_requests = []\n",
    "infer_responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 0, batch size 1\n",
      "Image '/workspace/images/mug.jpg':\n",
      "    504 (COFFEE MUG) = 0.7773650288581848\n"
     ]
    }
   ],
   "source": [
    "for infer_request in filledRequestGenerator(result_filenames):\n",
    "    caip_request = grpc_gcp_caip_pb2.CaipRequest(request_type=grpc_gcp_caip_pb2.TYPE_INFER_REQUEST, infer_request = infer_request)\n",
    "    infer_responses.append(grpc_service_pb2.InferResponse().FromString(requests.request(\"POST\", predict_url, headers=headers, data = caip_request.SerializeToString()).content))\n",
    "    \n",
    "idx = 0\n",
    "for infer_response in infer_responses:\n",
    "    print(\"Request {}, batch size {}\".format(idx, FLAGS.batch_size))\n",
    "    grpc_image_client.postprocess(infer_response.meta_data.output, result_filenames[idx], FLAGS.batch_size)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_text = \"gimme your money\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "request_type: TYPE_TRACER\n",
       "trace_message: \"gimme your money\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caip_tracer = grpc_gcp_caip_pb2.CaipRequest(request_type=grpc_gcp_caip_pb2.TYPE_TRACER, trace_message = tracer_text)\n",
    "caip_tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Your trace message was: gimme your money'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"POST\", predict_url, headers=headers, data = caip_tracer.SerializeToString()).content\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
